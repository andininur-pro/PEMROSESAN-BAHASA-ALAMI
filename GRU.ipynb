{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nWc3n-8-B2j1",
        "outputId": "da0b1ef7-aaf8-4bd4-d68c-8b071c0b170f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4w4xnQ2XUj6h",
        "outputId": "5124389d-7a26-4d9b-bc10-f60685fb3e54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oY44-m3rkWjr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filepath):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            split_line = line.strip().split('\\t')\n",
        "            if len(split_line) == 2:\n",
        "                texts.append(split_line[0])\n",
        "                try:\n",
        "                    labels.append(int(split_line[1]))\n",
        "                except ValueError:\n",
        "                    print(f\"Invalid label in line: {line}\")\n",
        "    return texts, labels\n"
      ],
      "metadata": {
        "id": "fXN6HZ7P1KHe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "HQzJGjM71PnL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correctly identify the text and label columns for train and test data\n",
        "text_column_train = train_df.columns[2]\n",
        "label_column_train = train_df.columns[0]\n",
        "text_column_test = test_df.columns[1]\n",
        "label_column_test = test_df.columns[0]\n",
        "\n",
        "# Access the data using the column names\n",
        "train_texts = train_df[text_column_train].tolist()\n",
        "train_labels = train_df[label_column_train].tolist()\n",
        "test_texts = test_df[text_column_test].tolist()\n",
        "test_labels = test_df[label_column_test].tolist()"
      ],
      "metadata": {
        "id": "GO4KIOiI1diZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing\n",
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "BniJJbTw3f3q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "train_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "IWHYGv2q4baU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "JBg7xXOr4hz6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to tensors\n",
        "y_train = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
        "y_test = torch.tensor(test_labels_encoded, dtype=torch.long)"
      ],
      "metadata": {
        "id": "EQAwNMna5VrZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe Embeddings\n",
        "print(\"Loading GloVe embeddings...\")\n",
        "EMBEDDING_DIM = 100\n",
        "glove_path = \"/content/glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Preparing embedding matrix...\")\n",
        "embedding_matrix = np.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < MAX_VOCAB_SIZE:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)"
      ],
      "metadata": {
        "id": "MPWdctiu5W5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4: Build GRU Model ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
        "        super(GRUClassifier, self).__init__()\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, hidden = self.gru(x)  # GRU only needs hidden state\n",
        "        x = self.fc(self.dropout(hidden[-1]))\n",
        "        return x"
      ],
      "metadata": {
        "id": "dsW9nwLA5zFx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(set(train_labels))  # Number of classes in the dataset\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = GRUClassifier(embedding_matrix, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)"
      ],
      "metadata": {
        "id": "dxh02kc755jD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 5: Train Model ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_sequences_tensor = torch.tensor(train_padded, dtype=torch.long)\n",
        "test_sequences_tensor = torch.tensor(test_padded, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(train_sequences_tensor, y_train)\n",
        "test_dataset = TensorDataset(test_sequences_tensor, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for sequences, labels in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sequences)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "uzbsVV0s6CJR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in loader:\n",
        "            output = model(sequences)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "jbA3Fbo86GMj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train(model, train_loader)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK25Whnh6H1c",
        "outputId": "187b9aff-4e41-4212-bd42-d5dc2e85bb80"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.9163, Test Accuracy: 0.7698\n",
            "Epoch 2/10, Loss: 0.3210, Test Accuracy: 0.8184\n",
            "Epoch 3/10, Loss: 0.2803, Test Accuracy: 0.8037\n",
            "Epoch 4/10, Loss: 0.2560, Test Accuracy: 0.7521\n",
            "Epoch 5/10, Loss: 0.2368, Test Accuracy: 0.7744\n",
            "Epoch 6/10, Loss: 0.2199, Test Accuracy: 0.7509\n",
            "Epoch 7/10, Loss: 0.2054, Test Accuracy: 0.7310\n",
            "Epoch 8/10, Loss: 0.1916, Test Accuracy: 0.7284\n",
            "Epoch 9/10, Loss: 0.1787, Test Accuracy: 0.7222\n",
            "Epoch 10/10, Loss: 0.1657, Test Accuracy: 0.7219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation\n",
        "final_accuracy = evaluate(model, test_loader)\n",
        "print(f\"Final Test Accuracy: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCwRJzzR6L8v",
        "outputId": "d9592418-baea-4667-c194-e8d272a44c68"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.7219\n"
          ]
        }
      ]
    }
  ]
}